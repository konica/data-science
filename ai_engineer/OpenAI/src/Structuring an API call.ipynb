{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac00d562",
   "metadata": {},
   "source": [
    "## Challenges when calling OPENAI API\n",
    "**Error Handling**\n",
    "- Displaying user-friendly error messages\n",
    "- Alternatives for when the service isunavailable\n",
    "\n",
    "**Moderation and Safety**\n",
    "- Control unwanted inputs\n",
    "- Minimizing the risk of data leaks\n",
    "\n",
    "**Testing and Validation**\n",
    "- Checking for responses that are out oftopic\n",
    "- Testing for inconsistent behavior\n",
    "\n",
    "**Communication with External Systems**\n",
    "- Calling external functions and APIs\n",
    "- Optimizing response times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb6ca8b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2819a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import json\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89b0e6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When calling the OpenAI API in a production environment, several challenges may arise, including:\n",
      "\n",
      "1. **Rate Limiting**: The OpenAI API has rate limits that restrict the number of requests you can make in a given time frame. Managing these limits is crucial to avoid interruptions in service and to ensure optimal performance.\n",
      "\n",
      "2. **Cost Management**: Using the API incurs costs based on usage. Monitoring and optimizing usage to prevent unexpected expenses can be challenging, especially for applications with variable traffic.\n",
      "\n",
      "3. **Latency**: Network latency can affect the responsiveness of your application. Ensuring quick API responses, especially in real-time applications, may require strategies such as caching frequent queries or optimizing the number of calls made.\n",
      "\n",
      "4. **Error Handling**: The API may return errors for various reasons (e.g., exceeding rate limits, invalid inputs). Implementing robust error handling and retry mechanisms is essential to maintain a smooth user experience.\n",
      "\n",
      "5. **Security and Privacy**: Protecting API keys and ensuring that sensitive data is not exposed during API calls is vital. This includes implementing proper authentication and using secure connections.\n",
      "\n",
      "6. **Model Updates**: OpenAI frequently updates its models and API specifications. Applications must be designed to handle potential changes in functionality or behaviors, requiring ongoing maintenance and testing.\n",
      "\n",
      "7. **Data Handling**: Ensuring that data sent to the API complies with privacy regulations (e.g., GDPR) and handling sensitive information appropriately is a significant challenge.\n",
      "\n",
      "8. **Integration Complexity**: Integrating the API into existing applications may require substantial changes to the codebase, particularly if the application was not designed with external API calls in mind.\n",
      "\n",
      "9. **Scalability**: As usage grows, ensuring that your application scales effectively to handle increased load without degrading performance can be challenging. This involves architectural considerations and possibly load balancing or distributed systems.\n",
      "\n",
      "10. **User Misinformation**: The OpenAI models can sometimes generate incorrect or misleading information. Implementing features that guide users or validate outputs is essential to mitigate this risk.\n",
      "\n",
      "Addressing these challenges typically involves thorough planning, comprehensive testing, and ongoing monitoring to adapt to changing conditions and usage patterns.\n"
     ]
    }
   ],
   "source": [
    "prompt= \"What challenges do you face when calling the OpenAI API in a production environment?\"\n",
    "repsonse = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "    # response_format={\"type\": \"json\", \"properties\": {\"content\": {\"type\": \"string\"}}}\n",
    ")\n",
    "\n",
    "print(repsonse.choices[0].message.content)  # extract message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009a514a",
   "metadata": {},
   "source": [
    "### Common errors\n",
    "**Connection errors**\n",
    "- Generally due to connection issues on either the user's or the service's side\n",
    "- Examples: InternalServerError, APIConnectionError, APITimeoutError\n",
    "- Solution: \n",
    "    - Checking your connection configuration, \n",
    "    - Reaching out to support if that fails\n",
    "\n",
    "**Resource limit errors**\n",
    "- Generally due limits on the frequency of requests or the amount of text passed\n",
    "- Examples: ConflictError, RateLimitError\n",
    "- Potential solution:\n",
    "    - Checking limit restrictions\n",
    "    - Ensure requests are within limits\n",
    "\n",
    "**Authentication errors**\n",
    "- Use invalid API key\n",
    "\n",
    "**Bad request errors**\n",
    "- Pass invalid role\n",
    "\n",
    "### Handle exceptions\n",
    "- To solve the errors above, your code need to handle exceptions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65465f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "\n",
    "try:     \n",
    "\tresponse = client.chat.completions.create(\n",
    "\t\tmodel=\"gpt-4o-mini\",\n",
    "\t\tmessages=[{\"role\": \"user\", \"content\": \"List five data science professions.\"}])\n",
    "except openai.AuthenticationError as e:\n",
    "\tprint(f\"OpenAI API failed to authenticate: {e}\")\n",
    "\tpass\n",
    "except openai.RateLimitError as e:\n",
    "\tprint(f\"OpenAI API request exceeded rate limit: {e}\")\n",
    "\tpass\n",
    "except Exception as e:\n",
    "\tprint(f\"Unable to generate a response. Exception: {e}\")\n",
    "\tpass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8430e02",
   "metadata": {},
   "source": [
    "# Batching\n",
    "- Combine multiple messages into a request to avoid rate limits due to calling multiple requests in a time unit like one minute.\n",
    "- Avoiding rate limits\n",
    "    - Retry: Short wait between requests\n",
    "    - Batching: Processing multiple messages in one request\n",
    "    - Reducing tokens: Quantifying and cutting down the number of tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce31f329",
   "metadata": {},
   "source": [
    "## Retrying with tenacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4f6ade7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"challenges\": {\n",
      "    \"1\": {\n",
      "      \"challenge\": \"Rate Limiting\",\n",
      "      \"description\": \"OpenAI API has usage limits that can affect the ability to handle high traffic or request bursts, requiring careful management of request flows.\"\n",
      "    },\n",
      "    \"2\": {\n",
      "      \"challenge\": \"Cost Management\",\n",
      "      \"description\": \"Frequent or intensive use of the API can lead to significant costs, necessitating budget tracking and cost optimization strategies.\"\n",
      "    },\n",
      "    \"3\": {\n",
      "      \"challenge\": \"Latency\",\n",
      "      \"description\": \"Response times can vary, impacting user experience, especially in real-time applications that demand low latency.\"\n",
      "    },\n",
      "    \"4\": {\n",
      "      \"challenge\": \"Error Handling\",\n",
      "      \"description\": \"Handling various types of errors, including timeouts, unexpected responses, or service downtime, to maintain application stability.\"\n",
      "    },\n",
      "    \"5\": {\n",
      "      \"challenge\": \"Security and Privacy\",\n",
      "      \"description\": \"Ensuring secure handling of user data and compliance with privacy regulations, especially when sending sensitive information to the API.\"\n",
      "    },\n",
      "    \"6\": {\n",
      "      \"challenge\": \"Model Behavior Variability\",\n",
      "      \"description\": \"The AI may produce inconsistent or unexpected results, requiring monitoring and potential filtering of outputs based on context.\"\n",
      "    },\n",
      "    \"7\": {\n",
      "      \"challenge\": \"Integration Complexity\",\n",
      "      \"description\": \"Integrating the API into existing systems or workflows can be complex, requiring significant development and testing efforts.\"\n",
      "    },\n",
      "    \"8\": {\n",
      "      \"challenge\": \"Dependency Management\",\n",
      "      \"description\": \"Reliance on a third-party service introduces potential risks, including service changes or deprecation, necessitating robust planning.\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Retrying\n",
    "from tenacity import (retry, stop_after_attempt, wait_random_exponential)\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "def get_response(model, message):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[message], \n",
    "        response_format={\"type\": \"json_object\"})\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "prompt= \"What challenges do you face when calling the OpenAI API in a production environment? Respond in JSON format.\"\n",
    "response = get_response(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    message={\"role\": \"user\", \"content\": prompt}\n",
    ")\n",
    "print(response)  # extract message\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b3a388",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa0439db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country: United States  \n",
      "Capital City: Washington, D.C.\n",
      "\n",
      "---\n",
      "\n",
      "Country: Ireland  \n",
      "Capital City: Dublin\n",
      "\n",
      "---\n",
      "\n",
      "Country: India  \n",
      "Capital City: New Delhi\n"
     ]
    }
   ],
   "source": [
    "countries = [\"United States\", \"Ireland\", \"India\"]\n",
    "message=[{\"role\": \"system\",\"content\": \"\"\"You are given a series of countries and are asked to return the country and capital city. Provide each of the questions with an answer in the response as separate content.\"\"\"}]\n",
    "\n",
    "[message.append({\"role\": \"user\", \"content\": i }) for i in countries]\n",
    "response = client.chat.completions.create(model=\"gpt-4o-mini\", messages=message)\n",
    "print(response.choices[0].message.content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f071fc",
   "metadata": {},
   "source": [
    "## Reducing tokens\n",
    "- Use `tiktoken` to count the number of tokens generated by a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5bc807",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "prompt = \"Tokens can be full words, or groups of characters commonly grouped together: tokenization.\"\n",
    "num_tokens = len(encoding.encode(prompt))\n",
    "print(\"Number of tokens in prompt:\", num_tokens)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
