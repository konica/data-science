{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ed93032",
   "metadata": {},
   "source": [
    "# Set up LLM model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7759e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "\n",
    "# os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9216210",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f691fb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93995cd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: psycopg[binary] in d:\\github\\open source\\data-science\\ai_engineer\\langgraph\\.venv\\lib\\site-packages (3.2.9)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in d:\\github\\open source\\data-science\\ai_engineer\\langgraph\\.venv\\lib\\site-packages (from psycopg[binary]) (4.14.1)\n",
      "Requirement already satisfied: tzdata in d:\\github\\open source\\data-science\\ai_engineer\\langgraph\\.venv\\lib\\site-packages (from psycopg[binary]) (2025.2)\n",
      "Collecting psycopg-binary==3.2.9 (from psycopg[binary])\n",
      "  Downloading psycopg_binary-3.2.9-cp311-cp311-win_amd64.whl.metadata (3.0 kB)\n",
      "Downloading psycopg_binary-3.2.9-cp311-cp311-win_amd64.whl (2.9 MB)\n",
      "   ---------------------------------------- 0.0/2.9 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.3/2.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.9/2.9 MB 12.2 MB/s eta 0:00:00\n",
      "Installing collected packages: psycopg-binary\n",
      "Successfully installed psycopg-binary-3.2.9\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install psycopg[binary]\n",
    "\n",
    "from langchain_postgres import PGVector\n",
    "import os\n",
    "connection = f\"postgresql+psycopg2://{os.getenv('POSTGRES_USERNAME_DB', 'postgres')}:{os.getenv('POSTGRES_PASSWORD_DB', 'Post!234')}@{os.getenv('POSTGRES_DATABASE_HOST', 'localhost')}:{os.getenv('POSTGRES_DATABASE_PORT', '5432')}/rag_agent_db\"\n",
    "\n",
    "vector_store = PGVector(\n",
    "    embeddings=embeddings,\n",
    "    collection_name=\"my_docs\",\n",
    "    connection=connection,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb17534",
   "metadata": {},
   "source": [
    "# Preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "892cb97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langsmith import Client\n",
    "\n",
    "# Load and chunk contents of the blog\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Index chunks\n",
    "# _ = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "# Define prompt for question-answering\n",
    "# N.B. for non-US LangSmith endpoints, you may need to specify\n",
    "# api_url=\"https://api.smith.langchain.com\" in hub.pull.\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "# prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "print(prompt) \n",
    "\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4abc489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"What is Task Decomposition?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a327c4d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know the answer because the context provided is empty.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Compare CoT and ToT?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879f1eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "# Create a LANGSMITH_API_KEY in Settings > API Keys\n",
    "from langsmith import Client\n",
    "client = Client(api_key=os.environ[\"LANGSMITH_API_KEY\"])\n",
    "prompt = client.pull_prompt(\"rlm/rag-prompt\", include_model=True)\n",
    "print(prompt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e80415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] input_types={} partial_variables={} metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'} messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "hub.pull(\"rlm/rag-prompt\")\n",
    "# prompt = hub.pull(\"rlm/rag-prompt\", api_key=os.environ[\"LANGSMITH_API_KEY\"], api_url=\"https://api.smith.langchain.com\")\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba0b77f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
