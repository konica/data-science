{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29199d5",
   "metadata": {},
   "source": [
    "# Overview\n",
    "- Learn about the structure of Hugging Face\n",
    "- Understand the difference between pipeline and automl\n",
    "- Distinguish encoder model, decoder model and encoder-decoder model\n",
    "- Capture common tasks in NLP\n",
    "    - Text classification\n",
    "    - Sentiment analyis\n",
    "    - Text summaration\n",
    "    - Text translation\n",
    "    - Text generation\n",
    "    - Question-answering problem\n",
    "- Evaluate the LLM response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd8d487",
   "metadata": {},
   "source": [
    "# What is Hugging Face\n",
    "- Be a home of the AI community where people share the open source models, datasets, applications\n",
    "- Have two types of inference: Local inference (Free and convenient but slow) and inference provider (fast and paid fee)\n",
    "- Hugging Face introduce the Transormers library to simplify working with pre-trained models. It includes two ways to use\n",
    "    - Use pipeline: easy and quick but be difficult to customize and adjust the settings \n",
    "    - Use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d3258",
   "metadata": {},
   "source": [
    "## Use pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74377752",
   "metadata": {},
   "source": [
    "### Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cb47708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'What if AI had made people do something about their own lives?\\n\\nAI, as a species, is evolving rapidly, and its capabilities are becoming increasingly sophisticated.\\n\\nBut that evolution is being accompanied by a fundamental change in our thinking. The concept of \"intelligence\" is no longer a technical concept, but a philosophical one. We are becoming more and more aware of the power of human thought, and our thinking is beginning to take this into account.\\n\\nWhat are the implications of this shift?\\n\\nMost people don\\'t realize the magnitude of this change. For example, humans are constantly in danger of being replaced by robots.\\n\\nBut many people don\\'t realize that, and many of us have already invested our lives in learning how to use algorithms or other technologies to help us find and solve problems that threaten our freedom and our safety.\\n\\nThe implications are profound.\\n\\nThe U.S. government has been using AI to help solve problems that we worry about, such as terrorism, or to help us solve a series of problems that we worry about.\\n\\nThe U.S. government has been using AI to help solve problems that we worry about, such as terrorism, or to help us solve a series of problems that we worry about. We are'}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "gpt2_pipeline = pipeline(task=\"text-generation\", model=\"openai-community/gpt2\")\n",
    "\n",
    "print(gpt2_pipeline(\"What if AI\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09a8a2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What if AI could have a really good heart? That could be\n",
      "What if AI was completely non-linear?\n",
      "\n",
      "A.\n"
     ]
    }
   ],
   "source": [
    "results = gpt2_pipeline(\"What if AI\", max_new_tokens=10, num_return_sequences=2)\n",
    "\n",
    "for result in results:\n",
    "    print(result['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e797534",
   "metadata": {},
   "source": [
    "### Text classification\n",
    "Some models to support this task\n",
    "- `abdulmatinomotoso/English_Grammar_Checker`: check grammar\n",
    "- `cross-encoder/qnli-electra-base`: check the answer associate with the question\n",
    "- `zero-shot-classification`: categorie texts into specific classes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e338e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\Open source\\data-science\\ai_engineer\\Hugging Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dtrun\\.cache\\huggingface\\hub\\models--abdulmatinomotoso--English_Grammar_Checker. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.9956323504447937}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\Open source\\data-science\\ai_engineer\\Hugging Face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Create a pipeline for grammar checking\n",
    "grammar_checker = pipeline(\n",
    "  task=\"text-classification\", \n",
    "  model=\"abdulmatinomotoso/English_Grammar_Checker\"\n",
    ")\n",
    "\n",
    "# Check grammar of the input text\n",
    "output = grammar_checker(\"I will walk dog\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60acb80e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_1', 'score': 0.9991169571876526}]\n"
     ]
    }
   ],
   "source": [
    "# Check grammar of the input text\n",
    "output = grammar_checker(\"I love you\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28d1d222",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\Open source\\data-science\\ai_engineer\\Hugging Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dtrun\\.cache\\huggingface\\hub\\models--cross-encoder--qnli-electra-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n",
      "d:\\GitHub\\Open source\\data-science\\ai_engineer\\Hugging Face\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `ElectraSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.016212010756134987}]\n"
     ]
    }
   ],
   "source": [
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"text-classification\", model=\"cross-encoder/qnli-electra-base\")\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(\"Where is the capital of France?, Brittany is known for its stunning coastline.\")\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a592e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\GitHub\\Open source\\data-science\\ai_engineer\\Hugging Face\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dtrun\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Label: science with score: 0.9510334730148315\n"
     ]
    }
   ],
   "source": [
    "text = \"AI-powered robots assist in complex brain surgeries with precision.\"\n",
    "\n",
    "# Create the pipeline\n",
    "classifier = pipeline(task=\"zero-shot-classification\", model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "# Create the categories list\n",
    "categories = [\"politics\", \"science\", \"sports\"]\n",
    "\n",
    "# Predict the output\n",
    "output = classifier(text, categories)\n",
    "\n",
    "# Print the top label and its score\n",
    "print(f\"Top Label: {output['labels'][0]} with score: {output['scores'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e2f9a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'POSITIVE', 'score': 0.9997085928916931}]\n"
     ]
    }
   ],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(task=\"text-classification\", model=\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "categories = [\"positive\", \"negative\", \"neutral\"]\n",
    "\n",
    "output = pipe(\"I love using Hugging Face!\")\n",
    "print(output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6211f5b9",
   "metadata": {},
   "source": [
    "### Text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2bd4e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text length: 831\n",
      "Summary length: 438\n",
      "Summary: the number of inhabited islands is diversely cited as between 166 and 227. The Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegesan islands, . a loose grouping off the west coast of Turkey; the Dodecanese, another loose collection in the southeast between C\n"
     ]
    }
   ],
   "source": [
    "original_text = \"'\\nGreece has many islands, with estimates ranging from somewhere around 1,200 to 6,000, depending on the minimum size to take into account. The number of inhabited islands is variously cited as between 166 and 227.\\nThe Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegean islands, a loose grouping off the west coast of Turkey; the Dodecanese, another loose collection in the southeast between Crete and Turkey; the Sporades, a small tight group off the coast of Euboea; and the Ionian Islands, chiefly located to the west of the mainland in the Ionian Sea. Crete with its surrounding islets and Euboea are traditionally excluded from this grouping.\\n'\"\n",
    "# Create the summarization pipeline\n",
    "summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\")\n",
    "\n",
    "# Summarize the text\n",
    "summary_text = summarizer(original_text)\n",
    "\n",
    "# Compare the length\n",
    "print(f\"Original text length: {len(original_text)}\")\n",
    "print(f\"Summary length: {len(summary_text[0]['summary_text'])}\")\n",
    "# Print the summary\n",
    "print(f\"Summary: {summary_text[0]['summary_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0e47f7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of inhabited islands is diversely \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of inhabited islands is diversely cited as between 166 and 227. The Greek islands are traditionally grouped into the following clusters: the Argo-Saronic Islands in the Saronic Gulf near Athens; the Cyclades, a large but dense collection occupying the central part of the Aegean Sea; the North Aegesan islands, . a loose grouping off the west coast of Turkey; the Dodecanese, another loose collection in the southeast between C\n"
     ]
    }
   ],
   "source": [
    "# Create a short summarizer\n",
    "short_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_new_tokens=1, max_new_tokens=10)\n",
    "\n",
    "# Summarize the input text\n",
    "short_summary_text = short_summarizer(original_text)\n",
    "\n",
    "# Print the short summary\n",
    "print(short_summary_text[0][\"summary_text\"])\n",
    "\n",
    "# Repeat for a long summarizer\n",
    "long_summarizer = pipeline(task=\"summarization\", model=\"cnicu/t5-small-booksum\", min_new_tokens=50, max_new_tokens=150)\n",
    "\n",
    "long_summary_text = long_summarizer(original_text)\n",
    "\n",
    "# Print the long summary\n",
    "print(long_summary_text[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b67d28f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.9266,  4.2142]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "inputs = tokenizer(\"I love using Hugging Face!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)   \n",
    "print(outputs.logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
